{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging an AzureML remote run\n",
    "\n",
    "This notebook shows how to an AzureML run that is running on and AMLCompute cluster. \n",
    "For setup instructions, please see the [Readme](README.md)\n",
    "\n",
    "\n",
    "First, we get the workspace and compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.41'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core import VERSION\n",
    "VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start run with debugger enabled\n",
    "\n",
    "First we will get the workspace and AML compute cluster we are about to use. The assumption is that you have created a cluster with the name `cpucluster` -- else change the name below accordingly. **It is important that, as you created the cluster, you have provided a username (I am using `debuguser`) and password and ssh key (ssh key is optional), since you will need to log in to the worker nodes to establish the port forwarding to the docker container.**\n",
    "\n",
    "![create_cluster](img/create_cluster.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "cluster = ws.compute_targets['cpucluster2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will start the job on the cluster. As a remote debugging agent, we will be using the Python Tools Visual Studio Debugger (PTVSD). As the debugging client, we will be using VSCode -- see [here](https://code.visualstudio.com/docs/python/debugging#_remote-debugging) for some documentation on how to use the two together.\n",
    "\n",
    "In our case we are using the command line launch method, since it doesn't require us to change our Python code. Instead I am adding the file `launch_debug.py` to the project, which takes the name of the actual script as a parameter and launches it from the debug agent. The agent will wait for the client to connect before running the actual script. The command line that `launch_debug.py` creates and then executes will look like this:\n",
    "\n",
    "    python -m ptvsd --host 10.0.0.4 --port 5678 --wait train.py\n",
    "    \n",
    "- `train.py` is the simple training script that we want to debug \n",
    "- `--host 10.0.0.4` means that the debugger will attach to this IP address on the worker (which wil a docker container in AML Compute) \n",
    "- `--port 5678` determines the port the agent will use \n",
    "- `--wait` instructs the agent to wait for the debugger to attach before proceeding with execution of the script\n",
    "\n",
    "The following code launches the script as shown above.\n",
    "\n",
    "In this example I am assuming there is a cluster `cpucluster` defined on the workspace. For the pip/conda dependencies, make sure to include the package `'ptvsd'` in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator('src',                                 # the directory where the launch and train script are\n",
    "                compute_target=cluster, \n",
    "                entry_script='launch_debug.py', \n",
    "                pip_packages=['sklearn', 'ptvsd'],     # make sure to include ptvsd in the list of pip packages\n",
    "                script_params={'': 'train.py'})\n",
    "\n",
    "run = Experiment(ws, 'debug').submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell will launch the run on the compute cluster, which will likely trigger a docker image to be created and then a compute node to be provision on the cluster. After about 9 minutes you should see the below image when you execute the next cell. This means that the process is now ready for the debugger to attach.\n",
    "\n",
    "![](img/waiting_for_debugger.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8492601cecd4c9dadd401000784c0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching the VSCode debugger\n",
    "\n",
    "### Establish the port-forwarding from Notebook VM to worker node\n",
    "Since Notebook VM does not yet support VNets, you need to build an SSH port forwarder through SSH login.\n",
    "\n",
    "First you need to find the IP and port of the node that is currently waiting for the debugger to connect. In the Azure portal in the Machine Learning Workspace, findthe nodes on the cluster by going to the nodes tab of the cluster. Note down the IP and port -- in my case it is `40.74.20.244` and port `50000`.\n",
    "\n",
    "![compute_nodes](img/compute_nodes.png)\n",
    "\n",
    "Then, open the terminal on the Notebook VM and type the following:  \n",
    "    \n",
    "    ssh debuguser@<clusternode IP> -p <clusternode port> -L 5678:<debugger IP>:5678\n",
    "\n",
    "In my case it is:\n",
    "    \n",
    "    ssh debuguser@40.74.20.244 -p 50000 -L 5678:10.0.0.4:5678\n",
    "\n",
    "Make sure to leave the terminal process running to keep the port-forward alive.\n",
    "\n",
    "If you want to double-check that the port-forward is indeed forwarding to the debug agent waiting for a connection just run this in another terminal on the Notebook VM:\n",
    "\n",
    "    telnet localhost 5678\n",
    "    \n",
    "This should return you something like this:\n",
    "\n",
    "    Trying 127.0.0.1...\n",
    "    Connected to localhost.\n",
    "    Escape character is '^]'.\n",
    "    Content-Length: 131\n",
    "\n",
    "    {\"type\": \"event\", \"seq\": 0, \"event\": \"output\", \"body\": {\"category\": \"telemetry\", \"output\": \"ptvsd\", \"data\": {\"version\": \"4.2.10\"}}}\n",
    "    Connection closed by foreign host.\n",
    "    \n",
    "    \n",
    "While setting up the port forward is tedious, you will only have to do it once for as long as your compute node stays up. If you set your cluster to have a higher value for **Idle seconds before scale down** (e.g. 3600 [i.e. 1 hour]), the you can work all day without having to re-establish the port-forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually attaching the VSCode debugger\n",
    "\n",
    "Now, all the hard work is done, and all that's left is open VSCode and remote into the Notebook VM (see [README.md](README.md) for setup instructions). Now, open the `src` folder of this repo as the project folder -- **it is important that the `src` folder is the project root in VSCode, so the file names match with the file names on the compute target**.\n",
    "\n",
    "Open the file `train.py` and set a breakpoint somewhere in the middle:\n",
    "\n",
    "![](img/set_breakpoint.png)\n",
    "\n",
    " Now attach the debugger by clicking on the debug icon on the left and then by picking the debug configuration \"Python: Remote Attach\" from the top. The debugger should attach and now allow you to step through your code.\n",
    " \n",
    " ![](img/debug.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036dc1ad76414463ad4f1ea94db5722e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est = Estimator('src',                                 # the directory where the launch and train script are\n",
    "                compute_target=cluster, \n",
    "                entry_script='launch_debug.py', \n",
    "                pip_packages=['sklearn', 'ptvsd'],     # make sure to include ptvsd in the list of pip packages\n",
    "                script_params={'': 'train.py'})\n",
    "\n",
    "run = Experiment(ws, 'debug').submit(est)\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
